Okay, I will now generate the revised Cross_Report_Implementation_Guide.md to reflect the new two-stage, human-in-the-loop approach for generating comparative reports.

This guide will leverage the Python package structure we've been developing (enhanced_report_generator) and detail how to achieve the initial scaffolding and then iteratively build a meaningful report.

Here is the revised guide:


# Cross-Report Implementation Guide (Revised)

## 1. Introduction: A New Iterative Approach

This document outlines a revised strategy for generating comprehensive cross-simulation comparative reports from Molecular Dynamics (MD) analyses. The goal is to combine robust data aggregation and automated scaffolding with human-in-the-loop iterative refinement, ensuring the final report is both insightful and tailored to specific research questions.

The previous system, while functional, sometimes produced sub-par reports when fully automated. This revised plan addresses that by:

VERY IMPORTANT : UNDER NO CIRCUMSTANCES DONT USE MOCK DATA OR FOLDBACK PROCEDURES. IF DATA IS MISSING OR ANALYSIS FAILS - THE USER SHOULD RECIEVE AN APPROPRIATE ERROR MESSAGE.

1.  **Stage 1: Foundational Scaffolding & Data Aggregation:** Automating the setup of all necessary data collection modules, AI integration placeholders, and the generation of a minimal initial HTML report. A key part of this stage is the aggregation of all relevant metrics from individual simulation analyses into a central database.
2.  **Stage 2: Human-Driven Iterative Enhancement:** Enabling a human expert to guide the generation of specific report sections, plots, and insights by providing instructions and code snippets that the AI Coder then integrates into the established framework.

This approach aims to maximize the strengths of both automation (for repetitive tasks and initial structure) and human expertise (for nuanced interpretation and customized presentation).

**Key Source Files for Context:**

* `Contributing.md`: Describes the "Pore Analysis Suite" which performs single-simulation analyses and generates individual `analysis_registry.db` files containing detailed metrics. These are the primary data sources.
* `Glossary.md`: Provides a list and description of all available metrics that can be collected and analyzed.
* `Scientific_background.md`: Offers domain-specific knowledge to inform AI-generated insights.

## 2. Overall Architecture: The `enhanced_report_generator` Package

The core of this system is the `enhanced_report_generator` Python package. Its modular design facilitates the two-stage approach.

**Proposed Package Structure:**

enhanced_report_generator/
├── init.py
├── main.py                     # Orchestrator for the report generation
│
├── core/
│   ├── init.py
│   └── data_models.py          # MetricResult, ReportSection, RunMetadata
│
├── components/
│   ├── init.py
│   ├── data_access.py          # DataRepository (reads aggregated data)
│   ├── data_aggregator.py      # New: For Stage 1 data collection
│   ├── analysis_selection.py   # AnalysisFilter, SectionBuilder
│   ├── visualization.py        # VisualizationEngine
│   ├── ai_insights.py          # AIInsightGenerator (initially Mock)
│   └── rendering.py            # ReportLayoutRenderer (HTML, PDF, Streamlit)
│
├── templates/
│   ├── init.py
│   └── report_template.html
│   └── system_overview_template.html # New: For initial minimal report
│
├── config/
│   ├── init.py
│   └── glossary_mapping.csv    # Maps metric prefixes to sections
│   └── report_config.yaml      # General report configurations
│
└── assets/
├── init.py
└── style.css               # Basic CSS for HTML reports


## 3. Stage 1: Data Aggregation and Scaffolding Setup

The primary goals of Stage 1 are to:
* Collect and consolidate all relevant data from individual simulation analyses.
* Set up the core software components (scaffolding).
* Generate a minimal initial HTML report containing only a "System Overview" tab.

### 3.1. Data Aggregation

A new component, `components/data_aggregator.py`, will be responsible for this crucial step.

**Responsibilities of `DataAggregator`:**

1.  **Locate Individual Databases:** Traverse specified root directories (containing folders for toxin-bound and toxin-free system groups) to find all individual `analysis_registry.db` files generated by the "Pore Analysis Suite".
2.  **Extract Metrics:** For each `analysis_registry.db` found:
    * Query the `metrics` table to extract all numerical metrics (as listed in `Glossary.md`).
    * Query the `simulation_metadata` table to extract relevant run information (e.g., `run_name`, `system_name`, `is_control_system`, `analysis_start_frame`, `analysis_end_frame`).
3.  **Consolidate Data:** Store the extracted metrics and metadata into a new, central SQLite database (e.g., `enhanced_cross_analysis.db`).

**Central Database Schema (`enhanced_cross_analysis.db` - Example):**

* **`systems` Table:**
    * `system_id` (INTEGER, PRIMARY KEY, AUTOINCREMENT)
    * `run_name` (TEXT, e.g., "R1", "R6_Toxin")
    * `group_type` (TEXT, e.g., "toxin-bound", "toxin-free")
    * `original_db_path` (TEXT, path to the source `analysis_registry.db`)
    * Other relevant metadata from `simulation_metadata`...
* **`aggregated_metrics` Table:**
    * `metric_id` (INTEGER, PRIMARY KEY, AUTOINCREMENT)
    * `system_id` (INTEGER, FOREIGN KEY REFERENCES `systems(system_id)`)
    * `metric_name` (TEXT, e.g., "DW_PROA_Open_Fraction", "Ion_AvgOcc_S1")
    * `value` (REAL)
    * `units` (TEXT, e.g., "%", "count", "Å")
    * `source_module` (TEXT, if available from original DB)

The `DataAggregator` will be run once initially to populate this central database. The `components/data_access.py::DataRepository` will then be configured to read from this `enhanced_cross_analysis.db` instead of a pre-existing (potentially simplified) `cross_analysis.db`.

### 3.2. Core Scaffolding Modules

The `enhanced_report_generator` package contains the following key modules that form the scaffolding:

* **`core/data_models.py`:** Defines core Python dataclasses (`MetricResult`, `ReportSection`, `RunMetadata`) used throughout the pipeline.
* **`components/data_access.py` (`DataRepository`):** Responsible for loading data *from the aggregated `enhanced_cross_analysis.db`*. It will also handle comparisons (e.g., calculating mean/std for toxin vs. control groups for each metric, performing statistical tests like p-value calculation).
* **`components/analysis_selection.py` (`AnalysisFilter`, `SectionBuilder`):**
    * `AnalysisFilter`: Initially, this might perform minimal filtering or be bypassed. Its full functionality (significance, effect size filtering) will be leveraged in Stage 2.
    * `SectionBuilder`: Initially, it will be configured to create only a single "System Overview" `ReportSection`.
* **`components/visualization.py` (`VisualizationEngine`):** In Stage 1, this module will likely not generate any complex plots. It might be called to produce a very simple overview graphic if needed for the "System Overview" tab, or simply return no plots.
* **`components/ai_insights.py` (`MockAIInsightGenerator`):** For Stage 1, this will be a mock implementation. For the "System Overview" tab, it can return a placeholder text like "This section will provide a high-level overview of the analyzed systems."
* **`components/rendering.py` (`ReportLayoutRenderer`):** This module will render the HTML report. A new, simplified Jinja2 template (e.g., `templates/system_overview_template.html`) will be used, or the main `report_template.html` will be modified to conditionally render only the "System Overview" section.

### 3.3. Generating the Initial "System Overview" Report

The `main.py` script will orchestrate Stage 1 as follows:

1.  **Run `DataAggregator` (once, or if source data changes):** Populate/update `enhanced_cross_analysis.db`.
2.  **Initialize `DataRepository`:** Point it to `enhanced_cross_analysis.db`.
3.  **Load Metadata:** `DataRepository` loads high-level metadata (e.g., number of toxin vs. control simulations, total frames analyzed per group).
4.  **Create "System Overview" Section:**
    * `SectionBuilder` (or `main.py` directly) creates a single `ReportSection` object.
    * Title: "System Overview".
    * Description: "Summary of simulation systems included in this comparative analysis."
    * Metrics: Populate with the high-level metadata.
    * Plots: None initially.
    * AI Interpretation: Placeholder text from `MockAIInsightGenerator`.
5.  **Render Report:** `ReportLayoutRenderer` uses the `system_overview_template.html` (or a conditional main template) to generate an HTML file with just this single tab/section.

**The output of Stage 1 is:**
* A populated `enhanced_cross_analysis.db`.
* A functional `enhanced_report_generator` package (the scaffolding).
* A simple HTML report with one tab: "System Overview", containing basic metadata.

## 4. Stage 2: Iterative Report Enhancement (Human-in-the-Loop)

With the data aggregated and the scaffolding in place, Stage 2 begins. This stage is an iterative process where a human expert guides the AI Coder to build out the detailed report.

### 4.1. Process Overview

1.  **Human Expert Provides Instructions:** The expert decides which comparisons are meaningful and what insights to highlight. They will provide specific instructions for new report sections, desired plots, and analytical focus.
2.  **AI Coder Implements Instructions:** The AI Coder modifies the `enhanced_report_generator` components to incorporate the expert's requirements.
3.  **Review and Iterate:** The expert reviews the generated report sections and provides feedback or new instructions. This loop continues until the report is satisfactory.

### 4.2. Types of Human Inputs

The human expert will provide inputs such as:

* **Definitions for New Report Sections:**
    * Example: "Create a section titled 'DW Gate Dynamics Comparison'. This section should analyze metrics prefixed with 'DW_' from the `Glossary.md`."
    * The AI Coder will update `config/glossary_mapping.csv` and potentially modify `SectionBuilder` logic.
* **Metric Selection and Filtering Logic:**
    * Example: "For the 'Ion Pathway Analysis' section, only include metrics with a p-value < 0.01 and a relative change greater than 20%. Prioritize 'Ion_HMM_ConductionEvents_Total'."
    * The AI Coder will adjust parameters or logic within `AnalysisFilter`.
* **Specifications or Code Snippets for Visualizations:**
    * Example: "For the 'DW Gate Dynamics' section, generate a bar plot comparing the 'DW_PROA_Open_Fraction' between toxin and control. Also, create a time series plot for 'DW_PROA_open_Mean_ns' if this data is directly available or can be computed."
    * The AI Coder will add new functions or modify existing ones in `VisualizationEngine`, ensuring plots are saved and registered correctly.
* **Prompts or Context for AI Insights:**
    * Example: "For the 'Tyrosine Rotamer Analysis' section, when prompting the AI, provide it with these specific sentences from `Scientific_background.md` concerning tyrosine's role in filter stability. Ask the AI to correlate changes in 'Tyr_HMM_Population\_' metrics with potential functional outcomes."
    * The AI Coder will enhance the prompt generation logic in `AIInsightGenerator` (once it's a real LLM integration) or refine the mock outputs.
* **Modifications to HTML Templates:**
    * Example: "In the 'Overall Summary' section, display the key findings as a bulleted list before the main AI interpretation paragraph."
    * The AI Coder will modify `templates/report_template.html`.

### 4.3. Role of AI Coder in Integrating Inputs

The AI Coder's role is to:

* Understand the expert's instructions.
* Translate these instructions into modifications of the Python code within the `enhanced_report_generator` package.
* Ensure that new sections, plots, and metrics are correctly integrated into the `ReportSection` data model.
* Verify that the `main.py` orchestrator correctly calls the modified components.
* Run the pipeline and provide the updated report to the expert for review.

## 5. Key Data Structures

Refer to `enhanced_report_generator/core/data_models.py` for the definitions of:

* `MetricResult`: Represents a single metric, including its values for control and toxin groups, difference, p-value, description, etc.
* `ReportSection`: Represents a thematic section in the report, containing a title, description, list of `MetricResult` objects, paths to plots, and an AI-generated interpretation.
* `RunMetadata`: Holds high-level metadata about the simulation runs being compared.

## 6. Configuration

* **`config/glossary_mapping.csv`:** Defines how metric name prefixes map to report section titles and descriptions. This will be heavily used and expanded in Stage 2.
    * Format: `prefix,section_title,section_description,metric_description_template`
* **`config/report_config.yaml` (or similar):** For storing global settings like p-value thresholds, effect size cutoffs, AI model parameters (for real AI), paths, etc.

## 7. Future Enhancements (Beyond Initial Implementation)

* **Real AI Integration:** Replace `MockAIInsightGenerator` with a class that interacts with a live LLM (e.g., via the Anthropic API, as in the original `ai_insights.py`). This involves robust prompt engineering, API key management, and error handling.
* **Advanced Visualization:** Implement a wider range of plot types in `VisualizationEngine` based on expert needs.
* **PDF and Streamlit Outputs:** Fully develop the `render_pdf_report` and `generate_streamlit_dashboard_data` methods in `ReportLayoutRenderer` for alternative output formats.
* **Dependency Tracking:** Implement more sophisticated dependency tracking if the analysis modules within the cross-report generation become complex and interdependent.

This revised guide provides a clear path for developing a flexible and powerful cross-simulation reporting tool that effectively combines automation with essential human expertise.

Sources and related content





Ok. now please provide the code elements required for a MVP implementing the above logic.


Okay, let's outline the code elements for an MVP (Minimum Viable Product) that implements Stage 1 of the revised plan:

Data Aggregation from individual analysis_registry.db files.
Generation of a minimal HTML report with a single "System Overview" tab.
This MVP will lay the foundation for the iterative human-in-the-loop process of Stage 2.

We'll need to:

Create the new components/data_aggregator.py.
Create the new templates/system_overview_template.html.
Modify components/data_access.py if needed to work with the new aggregated DB structure.
Adjust main.py to orchestrate this initial Stage 1 flow.
Ensure other components (analysis_selection.py, visualization.py, ai_insights.py, rendering.py) are either used minimally or their mock/placeholder behavior is suitable for this stage.
Here are the key code elements:

1. New: enhanced_report_generator/components/data_aggregator.py
This script will find individual analysis_registry.db files, extract data, and populate the new enhanced_cross_analysis.db.

Python

# enhanced_report_generator/components/data_aggregator.py
import sqlite3
import logging
from pathlib import Path
from typing import List, Dict, Any, Tuple

logger = logging.getLogger(__name__)

# Define the schema for the new aggregated database
AGGREGATED_DB_SCHEMA = {
    "systems": """
        CREATE TABLE IF NOT EXISTS systems (
            system_id INTEGER PRIMARY KEY AUTOINCREMENT,
            run_name TEXT NOT NULL UNIQUE, -- Assuming run_name from original DB is unique identifier
            group_type TEXT, -- 'toxin-bound' or 'toxin-free'
            original_db_path TEXT,
            psf_file TEXT,
            dcd_file TEXT,
            analysis_start_frame INTEGER,
            analysis_end_frame INTEGER,
            trajectory_total_frames INTEGER,
            is_control_system BOOLEAN
        )
    """,
    "aggregated_metrics": """
        CREATE TABLE IF NOT EXISTS aggregated_metrics (
            metric_id INTEGER PRIMARY KEY AUTOINCREMENT,
            system_id INTEGER NOT NULL,
            metric_name TEXT NOT NULL,
            value REAL,
            units TEXT,
            source_module TEXT, -- Optional: module that generated it in original DB
            FOREIGN KEY (system_id) REFERENCES systems (system_id)
        )
    """
}

class DataAggregator:
    def __init__(self, aggregated_db_path: Path):
        self.aggregated_db_path = aggregated_db_path
        self.conn = None
        self.cursor = None

    def _connect_db(self):
        """Connects to the aggregated database and creates tables if they don't exist."""
        self.conn = sqlite3.connect(self.aggregated_db_path)
        self.conn.row_factory = sqlite3.Row # Access columns by name
        self.cursor = self.conn.cursor()
        for table_name, schema_sql in AGGREGATED_DB_SCHEMA.items():
            try:
                self.cursor.execute(schema_sql)
            except sqlite3.Error as e:
                logger.error(f"Error creating table {table_name} in {self.aggregated_db_path}: {e}")
                raise
        self.conn.commit()
        logger.info(f"Successfully connected to and ensured schema in {self.aggregated_db_path}")

    def _close_db(self):
        if self.conn:
            self.conn.commit()
            self.conn.close()
            self.conn = None
            self.cursor = None

    def discover_individual_dbs(self, root_dirs: List[Path], db_filename: str = "analysis_registry.db") -> List[Path]:
        """Discovers individual analysis_registry.db files."""
        db_paths = []
        for root_dir in root_dirs:
            if not root_dir.is_dir():
                logger.warning(f"Root directory {root_dir} not found or not a directory. Skipping.")
                continue
            for db_path in root_dir.rglob(f"**/{db_filename}"): # Search in subdirectories of simulation runs
                if db_path.is_file():
                    db_paths.append(db_path)
        logger.info(f"Discovered {len(db_paths)} individual database files.")
        return db_paths

    def _extract_metadata_from_db(self, individual_db_conn: sqlite3.Connection) -> Dict[str, Any]:
        """Extracts relevant metadata from a single analysis_registry.db."""
        meta_cursor = individual_db_conn.cursor()
        metadata = {}
        try:
            # Fields from simulation_metadata table in individual DBs (as per Contributing.md)
            # Adjust keys based on actual content of simulation_metadata
            keys_to_extract = [
                'run_name', 'system_name', 'psf_file', 'dcd_file',
                'analysis_start_frame', 'analysis_end_frame',
                'trajectory_total_frames', 'is_control_system'
            ]
            for key in keys_to_extract:
                meta_cursor.execute("SELECT value FROM simulation_metadata WHERE key=?", (key,))
                row = meta_cursor.fetchone()
                if row:
                    # Handle boolean conversion for 'is_control_system'
                    if key == 'is_control_system':
                        metadata[key] = str(row['value']).lower() == 'true'
                    elif key in ['analysis_start_frame', 'analysis_end_frame', 'trajectory_total_frames']:
                         metadata[key] = int(row['value']) if row['value'] is not None else None
                    else:
                        metadata[key] = row['value']
                else:
                    metadata[key] = None
            # Determine group_type based on system_name or is_control_system
            if metadata.get('is_control_system') is True: # Explicitly check for True
                metadata['group_type'] = 'toxin-free'
            elif metadata.get('is_control_system') is False: # Explicitly check for False
                metadata['group_type'] = 'toxin-bound'
            else: # Fallback if is_control_system is not definitive or missing
                metadata['group_type'] = 'unknown'
                if 'toxin' in str(metadata.get('system_name','')).lower():
                     metadata['group_type'] = 'toxin-bound'
                elif 'control' in str(metadata.get('system_name','')).lower():
                     metadata['group_type'] = 'toxin-free'

        except sqlite3.Error as e:
            logger.error(f"Error extracting metadata: {e}")
        return metadata

    def _extract_metrics_from_db(self, individual_db_conn: sqlite3.Connection) -> List[Tuple[str, float, str, str]]:
        """Extracts metrics from a single analysis_registry.db."""
        metrics_cursor = individual_db_conn.cursor()
        metrics_data = []
        try:
            # Fields from metrics table in individual DBs
            metrics_cursor.execute("SELECT metric_name, value, units, module_name FROM metrics")
            for row in metrics_cursor.fetchall():
                metrics_data.append((row['metric_name'], row['value'], row['units'], row['module_name']))
        except sqlite3.Error as e:
            logger.error(f"Error extracting metrics: {e}")
        return metrics_data

    def aggregate_data(self, individual_db_paths: List[Path], clear_existing_aggregated_data: bool = False):
        """Aggregates data from individual DBs into the central aggregated DB."""
        if not self.conn or not self.cursor:
            self._connect_db()

        if clear_existing_aggregated_data:
            logger.info("Clearing existing data from aggregated_metrics and systems tables.")
            self.cursor.execute("DELETE FROM aggregated_metrics")
            self.cursor.execute("DELETE FROM systems")
            self.conn.commit()

        for db_path in individual_db_paths:
            logger.info(f"Processing individual database: {db_path}")
            try:
                ind_conn = sqlite3.connect(db_path)
                ind_conn.row_factory = sqlite3.Row # Access columns by name

                metadata = self._extract_metadata_from_db(ind_conn)
                run_name = metadata.get('run_name')
                if not run_name:
                    logger.warning(f"Skipping DB {db_path} as 'run_name' not found in metadata.")
                    ind_conn.close()
                    continue

                # Insert or update system metadata
                self.cursor.execute("SELECT system_id FROM systems WHERE run_name = ?", (run_name,))
                system_row = self.cursor.fetchone()
                system_id = None

                system_data_tuple = (
                    run_name, metadata.get('group_type'), str(db_path),
                    metadata.get('psf_file'), metadata.get('dcd_file'),
                    metadata.get('analysis_start_frame'), metadata.get('analysis_end_frame'),
                    metadata.get('trajectory_total_frames'), metadata.get('is_control_system')
                )

                if system_row:
                    system_id = system_row['system_id']
                    logger.debug(f"System '{run_name}' already exists with ID {system_id}. Metrics will be added/updated.")
                    # Optionally, update metadata if needed:
                    # self.cursor.execute("""
                    # UPDATE systems SET group_type=?, original_db_path=?, ... WHERE system_id=?
                    # """, (..., system_id))
                else:
                    self.cursor.execute("""
                        INSERT INTO systems (
                            run_name, group_type, original_db_path, psf_file, dcd_file,
                            analysis_start_frame, analysis_end_frame, trajectory_total_frames,
                            is_control_system
                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, system_data_tuple)
                    system_id = self.cursor.lastrowid
                    logger.info(f"Inserted new system '{run_name}' with ID {system_id}.")

                # Extract and insert metrics
                metrics = self._extract_metrics_from_db(ind_conn)
                metrics_to_insert = []
                for metric_name, value, units, source_module in metrics:
                    metrics_to_insert.append((system_id, metric_name, value, units, source_module))

                if metrics_to_insert:
                    # For simplicity, let's delete old metrics for this system_id if re-aggregating
                    # A more robust approach might involve updating existing metrics.
                    self.cursor.execute("DELETE FROM aggregated_metrics WHERE system_id = ?", (system_id,))
                    self.cursor.executemany("""
                        INSERT INTO aggregated_metrics (system_id, metric_name, value, units, source_module)
                        VALUES (?, ?, ?, ?, ?)
                    """, metrics_to_insert)
                    logger.info(f"Inserted {len(metrics_to_insert)} metrics for system ID {system_id} ('{run_name}').")

                ind_conn.close()
                self.conn.commit()

            except sqlite3.Error as e:
                logger.error(f"SQLite error processing {db_path}: {e}")
            except Exception as e:
                logger.error(f"General error processing {db_path}: {e}")
        logger.info("Data aggregation complete.")

    def run(self, root_dirs_for_discovery: List[Path], clear_existing: bool = True):
        """Main execution method for the aggregator."""
        self._connect_db()
        try:
            individual_dbs = self.discover_individual_dbs(root_dirs_for_discovery)
            if individual_dbs:
                self.aggregate_data(individual_dbs, clear_existing_aggregated_data=clear_existing)
            else:
                logger.warning("No individual databases found. Aggregation skipped.")
        finally:
            self._close_db()

if __name__ == '__main__':
    # Example usage:
    logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(name)s: %(message)s")
    
    # These paths should point to the directories containing your 'toxin-bound' and 'toxin-free'
    # simulation run folders, where each run folder has an 'analysis_registry.db'
    # For MVP testing, you might need to create dummy analysis_registry.db files.
    sim_root_dirs = [
        Path("./example_sim_runs/toxin_systems"),    # Contains runA/analysis_registry.db, runB/analysis_registry.db
        Path("./example_sim_runs/control_systems") # Contains runC/analysis_registry.db, runD/analysis_registry.db
    ]
    
    # Create dummy individual DBs for testing if they don't exist
    # In a real scenario, these are produced by the "Pore Analysis Suite"
    for root_dir_type in sim_root_dirs:
        for i in range(1, 3): # Create two dummy runs per type
            run_name = f"dummy_run_{root_dir_type.name}_{i}"
            dummy_run_path = root_dir_type / run_name
            dummy_run_path.mkdir(parents=True, exist_ok=True)
            dummy_db_path = dummy_run_path / "analysis_registry.db"
            if not dummy_db_path.exists():
                conn_ind = sqlite3.connect(dummy_db_path)
                cur_ind = conn_ind.cursor()
                cur_ind.execute("CREATE TABLE IF NOT EXISTS simulation_metadata (key TEXT, value TEXT)")
                cur_ind.execute("CREATE TABLE IF NOT EXISTS metrics (metric_name TEXT, value REAL, units TEXT, module_name TEXT)")
                is_control = "control" in root_dir_type.name
                cur_ind.executemany("INSERT INTO simulation_metadata VALUES (?,?)", [
                    ('run_name', run_name),
                    ('system_name', root_dir_type.name),
                    ('is_control_system', str(is_control)),
                    ('analysis_start_frame', 0),
                    ('analysis_end_frame', 1000 if i==1 else 2000),
                ])
                cur_ind.executemany("INSERT INTO metrics VALUES (?,?,?,?)", [
                    ('DW_PROA_Open_Fraction', 50.0 + i*5, '%', 'dw_gate_analysis'),
                    ('Ion_AvgOcc_S1', 0.75 - i*0.1, 'count', 'ion_analysis'),
                    ('COM_Mean_Filt', 10.0 + i, 'Å', 'core_analysis_filtering')
                ])
                conn_ind.commit()
                conn_ind.close()
                logger.info(f"Created dummy individual DB: {dummy_db_path}")


    aggregator = DataAggregator(aggregated_db_path=Path("./enhanced_cross_analysis.db"))
    aggregator.run(root_dirs_for_discovery=sim_root_dirs, clear_existing=True)
    logger.info(f"Aggregated data written to {aggregator.aggregated_db_path}")

    # To verify, you can open enhanced_cross_analysis.db with a SQLite browser
2. New: enhanced_report_generator/templates/system_overview_template.html
A very simple template for the initial "System Overview" report.

HTML

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{{ report_title }} - System Overview</title>
    <link rel="stylesheet" href="assets/style.css"> </head>
<body>
    <header>
        <h1>{{ report_title }}</h1>
        <p class="timestamp">Report Generated: {{ generation_timestamp }}</p>
        {% if run_metadata %}
            <p>
                Comparing System Groups:
                {% if run_metadata.toxin_run_ids and run_metadata.toxin_run_ids|length > 0 %}
                    {{ run_metadata.toxin_run_ids|length }} Toxin-Bound System(s)
                {% endif %}
                {% if run_metadata.control_run_ids and run_metadata.control_run_ids|length > 0 %}
                    vs. {{ run_metadata.control_run_ids|length }} Toxin-Free (Control) System(s)
                {% endif %}
            </p>
             <p>Toxin Name (if applicable): {{ run_metadata.toxin_name | default('N/A') }}</p>
             <p>Channel Name: {{ run_metadata.channel_name | default('N/A') }}</p>
        {% endif %}
    </header>

    <section class="section">
        <h2>System Overview</h2>
        {% if section and section.description %}
            <p class="section-description">{{ section.description }}</p>
        {% endif %}

        {% if section and section.metrics %}
            <h3>High-Level Summary</h3>
            <table class="metrics-table">
                <thead>
                    <tr>
                        <th>Parameter</th>
                        <th>Value</th>
                    </tr>
                </thead>
                <tbody>
                    {% for metric in section.metrics %}
                    <tr>
                        <td>{{ metric.name }}</td>
                        <td>
                            {{ metric.value_toxin if metric.value_toxin is not none else metric.value_control }}
                            {{ metric.units }}
                        </td>
                    </tr>
                    {% endfor %}
                </tbody>
            </table>
        {% else %}
            <p>No summary metrics available for this overview.</p>
        {% endif %}

        {% if section and section.ai_interpretation %}
            <h3>AI-Generated Interpretation</h3>
            <div class="ai-interpretation">
                {{ section.ai_interpretation }}
            </div>
        {% endif %}
    </section>

    <footer class="footer">
        <p>Enhanced Report Generation System - Stage 1 MVP</p>
        <p>Timestamp: {{ generation_timestamp }}</p>
    </footer>
</body>
</html>
3. Modify: enhanced_report_generator/components/data_access.py
The DataRepository needs to be adapted to read from the new enhanced_cross_analysis.db and perform the necessary comparisons (toxin vs. control).

Python

# enhanced_report_generator/components/data_access.py
import sqlite3
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any, Optional
from ..core.data_models import MetricResult, RunMetadata # Assuming these are defined in core.data_models
import numpy as np
from scipy import stats # For p-value calculation

logger = logging.getLogger(__name__)

class DataRepository:
    def __init__(self, db_path: Path):
        self.db_path = db_path
        if not self.db_path.exists():
            raise FileNotFoundError(f"Aggregated database not found: {self.db_path}. Please run DataAggregator first.")
        logger.info(f"DataRepository initialized with DB: {self.db_path}")

    def _get_system_ids_by_group(self, group_type: str) -> List[int]:
        """Helper to get system_ids for a given group_type."""
        ids = []
        try:
            conn = sqlite3.connect(self.db_path)
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            cursor.execute("SELECT system_id FROM systems WHERE group_type = ?", (group_type,))
            ids = [row['system_id'] for row in cursor.fetchall()]
            conn.close()
        except sqlite3.Error as e:
            logger.error(f"Database error while fetching system IDs for group {group_type}: {e}")
        return ids

    def load_run_metadata(self) -> RunMetadata:
        """Loads high-level metadata about the compared runs from the aggregated DB."""
        logger.info(f"Loading run metadata from {self.db_path}")
        control_run_names: List[str] = []
        toxin_run_names: List[str] = []
        # For simplicity, we'll just count runs. More detailed metadata could be fetched.
        # We can also try to get a common toxin/channel name if consistent.
        toxin_name_common = None
        channel_name_common = None # Placeholder

        try:
            conn = sqlite3.connect(self.db_path)
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()

            cursor.execute("SELECT run_name FROM systems WHERE group_type = 'toxin-free'")
            control_run_names = [row['run_name'] for row in cursor.fetchall()]

            cursor.execute("SELECT run_name FROM systems WHERE group_type = 'toxin-bound'")
            toxin_run_names = [row['run_name'] for row in cursor.fetchall()]
            
            # Try to get toxin_name (e.g. if system_name in individual DBs was 'toxin_XYZ-run1')
            # This part is a heuristic and might need refinement based on actual 'system_name' conventions.
            if toxin_run_names:
                cursor.execute("SELECT original_db_path FROM systems WHERE run_name = ?", (toxin_run_names[0],))
                first_toxin_db_path_row = cursor.fetchone()
                if first_toxin_db_path_row:
                    # This requires reconnecting to the original DB to get system_name
                    # Or assuming system_name was stored in the aggregated DB's systems table.
                    # For MVP, let's assume 'toxin_name' might be part of the run_name or system_name directly.
                    # This is simplified. A better approach might be to query for a common 'toxin_identifier'
                    # if it were stored during aggregation.
                    pass # Simplified for MVP

            conn.close()
        except sqlite3.Error as e:
            logger.error(f"Database error loading run metadata: {e}")

        return RunMetadata(
            control_run_ids=control_run_names,
            toxin_run_ids=toxin_run_names,
            toxin_name=toxin_name_common or "UnknownToxin", # Default
            channel_name=channel_name_common or "UnknownChannel", # Default
            simulation_parameters={} # Could be aggregated if consistent
        )

    def load_metrics_for_stage1_overview(self) -> List[MetricResult]:
        """
        Loads very high-level summary data for the Stage 1 "System Overview" tab.
        This doesn't perform toxin vs control comparison but gives counts.
        """
        logger.info(f"Loading high-level metrics for Stage 1 overview from {self.db_path}")
        overview_metrics = []
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            cursor.execute("SELECT COUNT(*) FROM systems WHERE group_type = 'toxin-bound'")
            num_toxin_systems = cursor.fetchone()[0]
            overview_metrics.append(MetricResult(name="Number of Toxin-Bound Systems", value_toxin=float(num_toxin_systems), value_control=0, units="systems", difference=0))

            cursor.execute("SELECT COUNT(*) FROM systems WHERE group_type = 'toxin-free'")
            num_control_systems = cursor.fetchone()[0]
            overview_metrics.append(MetricResult(name="Number of Toxin-Free (Control) Systems", value_toxin=float(num_control_systems), value_control=0, units="systems", difference=0))
            
            # Example: Average trajectory length for toxin systems
            cursor.execute("SELECT AVG(trajectory_total_frames) FROM systems WHERE group_type = 'toxin-bound'")
            avg_frames_toxin_row = cursor.fetchone()
            if avg_frames_toxin_row and avg_frames_toxin_row[0] is not None:
                 overview_metrics.append(MetricResult(name="Avg. Trajectory Frames (Toxin)", value_toxin=float(avg_frames_toxin_row[0]), value_control=0, units="frames", difference=0))

            cursor.execute("SELECT AVG(trajectory_total_frames) FROM systems WHERE group_type = 'toxin-free'")
            avg_frames_control_row = cursor.fetchone()
            if avg_frames_control_row and avg_frames_control_row[0] is not None:
                 overview_metrics.append(MetricResult(name="Avg. Trajectory Frames (Control)", value_toxin=float(avg_frames_control_row[0]), value_control=0, units="frames", difference=0))

            conn.close()
        except sqlite3.Error as e:
            logger.error(f"Database error loading overview metrics: {e}")
        
        return overview_metrics


    def load_and_compare_metrics(self) -> List[MetricResult]:
        """
        Fetches metrics from the aggregated DB, performs comparisons between
        toxin and control groups, and returns a list of MetricResult objects.
        This is for Stage 2 and detailed reports.
        """
        logger.info(f"Loading and comparing metrics from {self.db_path}")
        compared_metrics: List[MetricResult] = []

        toxin_system_ids = self._get_system_ids_by_group('toxin-bound')
        control_system_ids = self._get_system_ids_by_group('toxin-free')

        if not toxin_system_ids or not control_system_ids:
            logger.warning("Not enough systems in both toxin and control groups for comparison. Aborting metric comparison.")
            return []

        try:
            conn = sqlite3.connect(self.db_path)
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()

            # Get all unique metric names
            cursor.execute("SELECT DISTINCT metric_name, units FROM aggregated_metrics")
            unique_metric_specs = [(row['metric_name'], row['units']) for row in cursor.fetchall()]

            for metric_name, units in unique_metric_specs:
                # Fetch values for toxin group
                cursor.execute(f"""
                    SELECT value FROM aggregated_metrics
                    WHERE metric_name = ? AND system_id IN ({','.join(['?']*len(toxin_system_ids))})
                """, (metric_name, *toxin_system_ids))
                toxin_values = [row['value'] for row in cursor.fetchall() if row['value'] is not None]

                # Fetch values for control group
                cursor.execute(f"""
                    SELECT value FROM aggregated_metrics
                    WHERE metric_name = ? AND system_id IN ({','.join(['?']*len(control_system_ids))})
                """, (metric_name, *control_system_ids))
                control_values = [row['value'] for row in cursor.fetchall() if row['value'] is not None]

                if not toxin_values or not control_values:
                    logger.debug(f"Skipping metric '{metric_name}': not enough data in one or both groups for comparison.")
                    continue

                mean_toxin = np.mean(toxin_values)
                mean_control = np.mean(control_values)
                difference = mean_toxin - mean_control
                
                p_value = None
                significant = False
                # Perform t-test if enough samples (e.g., >1 in each group, though more is better)
                if len(toxin_values) > 1 and len(control_values) > 1:
                    # Welch's t-test (assumes unequal variances by default)
                    ttest_result = stats.ttest_ind(toxin_values, control_values, equal_var=False, nan_policy='omit')
                    p_value = ttest_result.pvalue
                    # Significance threshold can be configured, e.g., 0.05
                    if p_value < 0.05: # Placeholder, should be configurable
                        significant = True
                else:
                    logger.debug(f"Skipping p-value calculation for '{metric_name}' due to insufficient sample size in one/both groups.")

                compared_metrics.append(
                    MetricResult(
                        name=metric_name,
                        value_control=float(mean_control),
                        value_toxin=float(mean_toxin),
                        units=units or "",
                        difference=float(difference),
                        p_value=float(p_value) if p_value is not None else None,
                        significant=significant,
                        # description and priority_score to be filled later by AnalysisFilter/SectionBuilder
                    )
                )
            conn.close()
        except sqlite3.Error as e:
            logger.error(f"Database error during metric loading and comparison: {e}")
        except Exception as general_exception:
            logger.error(f"General error during metric loading and comparison: {general_exception}")


        if not compared_metrics:
             logger.warning("No metrics were successfully compared. Detailed report might be empty.")
        else:
             logger.info(f"Successfully compared {len(compared_metrics)} unique metrics.")
        return compared_metrics

4. Modify: enhanced_report_generator/main.py
The main.py needs a way to distinguish Stage 1 execution and to call the DataAggregator.

Python

# enhanced_report_generator/main.py
import argparse
import logging
from pathlib import Path
import shutil

from .core.data_models import ReportSection # RunMetadata will be loaded by DataRepository
from .components.data_aggregator import DataAggregator
from .components.data_access import DataRepository
from .components.analysis_selection import AnalysisFilter, SectionBuilder
from .components.ai_insights import MockAIInsightGenerator
from .components.visualization import VisualizationEngine
from .components.rendering import ReportLayoutRenderer

BASE_DIR = Path(__file__).resolve().parent
DEFAULT_OUTPUT_DIR = BASE_DIR.parent / "enhanced_report_output"
DEFAULT_TEMPLATES_DIR = BASE_DIR / "templates"
DEFAULT_CONFIG_DIR = BASE_DIR / "config"
DEFAULT_GLOSSARY_PATH = DEFAULT_CONFIG_DIR / "glossary_mapping.csv"
DEFAULT_ASSETS_DIR = BASE_DIR / "assets"
DEFAULT_AGGREGATED_DB_NAME = "enhanced_cross_analysis.db"

# --- Dummy file creation (ensure_dummy_glossary, ensure_dummy_template, ensure_assets) ---
# (Keep these from the previous main.py, they help ensure the MVP can run)
def ensure_dummy_glossary(glossary_path: Path):
    if not glossary_path.exists():
        glossary_path.parent.mkdir(parents=True, exist_ok=True)
        with open(glossary_path, "w", newline="") as f:
            f.write("prefix,section_title,section_description,metric_description_template\n")
            # For Stage 1, this glossary might not be used extensively for sectioning
            f.write("Overview,System Overview,\"High-level summary of analyzed systems.\",\"Parameter {metric_name}: {value_toxin} {units}\"\n")
        logging.info(f"Created dummy glossary at {glossary_path}")

def ensure_system_overview_template(template_dir: Path, template_name: str = "system_overview_template.html"):
    template_path = template_dir / template_name
    if not template_path.exists():
        template_dir.mkdir(parents=True, exist_ok=True)
        # Content of system_overview_template.html (as provided above)
        html_content = """
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{{ report_title }} - System Overview</title>
    <link rel="stylesheet" href="assets/style.css">
</head>
<body>
    <header>
        <h1>{{ report_title }}</h1>
        <p class="timestamp">Report Generated: {{ generation_timestamp }}</p>
        {% if run_metadata %}
            <p>
                Comparing System Groups:
                {% if run_metadata.toxin_run_ids and run_metadata.toxin_run_ids|length > 0 %}
                    {{ run_metadata.toxin_run_ids|length }} Toxin-Bound System(s)
                {% endif %}
                {% if run_metadata.control_run_ids and run_metadata.control_run_ids|length > 0 %}
                    vs. {{ run_metadata.control_run_ids|length }} Toxin-Free (Control) System(s)
                {% endif %}
            </p>
             <p>Toxin Name (if applicable): {{ run_metadata.toxin_name | default('N/A') }}</p>
             <p>Channel Name: {{ run_metadata.channel_name | default('N/A') }}</p>
        {% endif %}
    </header>
    <section class="section">
        <h2>System Overview</h2>
        {% if section and section.description %}
            <p class="section-description">{{ section.description }}</p>
        {% endif %}
        {% if section and section.metrics %}
            <h3>High-Level Summary</h3>
            <table class="metrics-table">
                <thead><tr><th>Parameter</th><th>Value</th></tr></thead>
                <tbody>
                    {% for metric in section.metrics %}
                    <tr><td>{{ metric.name }}</td><td>{{ metric.value_toxin if metric.value_toxin is not none else metric.value_control }} {{ metric.units }}</td></tr>
                    {% endfor %}
                </tbody>
            </table>
        {% else %}<p>No summary metrics available.</p>{% endif %}
        {% if section and section.ai_interpretation %}<div class="ai-interpretation"><h3>AI Interpretation</h3>{{ section.ai_interpretation }}</div>{% endif %}
    </section>
    <footer class="footer"><p>Enhanced Report - Stage 1 MVP. {{ generation_timestamp }}</p></footer>
</body>
</html>"""
        with open(template_path, "w", encoding="utf-8") as f:
            f.write(html_content)
        logging.info(f"Created system_overview_template.html at {template_path}")

def ensure_assets(assets_dir: Path):
    style_css_path = assets_dir / "style.css"
    if not style_css_path.exists():
        assets_dir.mkdir(parents=True, exist_ok=True)
        style_content = """
        body { font-family: Arial, sans-serif; margin: 20px; line-height: 1.6; color: #333; }
        header { border-bottom: 2px solid #007bff; padding-bottom: 10px; margin-bottom: 20px; }
        h1 { color: #007bff; } h2 { color: #333; border-bottom: 1px solid #ccc; padding-bottom: 5px; margin-top: 30px; }
        .timestamp { font-size: 0.9em; color: #777; text-align: right; }
        .section { margin-bottom: 40px; padding: 15px; border: 1px solid #ddd; border-radius: 5px; background-color: #f9f9f9;}
        .section-description { font-style: italic; color: #555; margin-bottom: 15px; }
        .metrics-table { width: 100%; border-collapse: collapse; margin-bottom: 20px; }
        .metrics-table th, .metrics-table td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        .metrics-table th { background-color: #f0f0f0; }
        .ai-interpretation { background-color: #f0f8ff; padding: 10px; border: 1px dashed #add8e6; border-radius: 4px; white-space: pre-wrap; margin-top:15px;}
        .footer { margin-top: 30px; text-align: center; font-size: 0.8em; color: #888; }
        """
        with open(style_css_path, "w") as f:
            f.write(style_content)
        logging.info(f"Created dummy style.css at {style_css_path}")

# --- End of dummy file creation ---

def setup_logging():
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S"
    )
    logging.getLogger('matplotlib').setLevel(logging.WARNING)

def run_stage1_pipeline(output_dir: Path, aggregated_db_name: str, report_title: str, individual_sim_roots: List[Path]):
    """Orchestrates Stage 1: Data Aggregation and System Overview Report."""
    logger = logging.getLogger(__name__)
    logger.info("--- Starting Stage 1: Data Aggregation & System Overview Report ---")

    output_dir.mkdir(parents=True, exist_ok=True)
    aggregated_db_path = output_dir / aggregated_db_name
    plots_output_dir = output_dir / "plots" # May not be used much in Stage 1
    plots_output_dir.mkdir(parents=True, exist_ok=True)

    # Ensure dummy files for a smooth MVP run
    ensure_dummy_glossary(DEFAULT_GLOSSARY_PATH)
    ensure_system_overview_template(DEFAULT_TEMPLATES_DIR)
    ensure_assets(DEFAULT_ASSETS_DIR)

    # --- 1. Data Aggregation ---
    if individual_sim_roots: # Only run aggregator if paths are provided
        logger.info("Running Data Aggregator...")
        aggregator = DataAggregator(aggregated_db_path=aggregated_db_path)
        aggregator.run(root_dirs_for_discovery=individual_sim_roots, clear_existing=True)
    else:
        logger.warning("No individual simulation root directories provided. Skipping data aggregation. "
                       f"Ensure '{aggregated_db_path}' exists if you intend to generate a report.")

    if not aggregated_db_path.exists():
        logger.error(f"Aggregated database {aggregated_db_path} not found and not created. Cannot proceed.")
        return

    # --- 2. Initialize Components for Overview Report ---
    logger.info("Initializing components for System Overview report...")
    data_repo = DataRepository(db_path=aggregated_db_path)
    # SectionBuilder isn't strictly needed if we manually create the overview section
    # ai_generator will be used for a placeholder text
    ai_generator = MockAIInsightGenerator(
        glossary_csv_path=DEFAULT_GLOSSARY_PATH # For API consistency
    )
    report_renderer = ReportLayoutRenderer(
        templates_dir=DEFAULT_TEMPLATES_DIR,
        output_dir=output_dir,
        assets_dir=DEFAULT_ASSETS_DIR
    )

    # --- 3. Load Data for Overview ---
    logger.info("Loading data for System Overview...")
    run_meta = data_repo.load_run_metadata()
    overview_summary_metrics = data_repo.load_metrics_for_stage1_overview()

    # --- 4. Create "System Overview" Report Section ---
    system_overview_section = ReportSection(
        title="System Overview",
        description="This section provides a high-level summary of the simulation systems included in this comparative analysis.",
        metrics=overview_summary_metrics,
        plots=[], # No plots for initial overview
        ai_interpretation=ai_generator.generate_section_insight(
            ReportSection(title="System Overview", metrics=overview_summary_metrics), # Pass a minimal section for context
            run_meta # Pass run_meta for context
        ) if overview_summary_metrics else "No system data loaded to generate AI overview."
    )

    # --- 5. Render HTML Report ---
    logger.info("Rendering System Overview HTML report...")
    html_file_path = report_renderer.render_html_report(
        report_title=f"{report_title} - System Overview",
        sections=[system_overview_section], # Pass as a list
        run_metadata=run_meta,
        global_ai_summary=None, # No global summary for this initial report
        template_name="system_overview_template.html", # Use the specific template
        output_filename=f"{report_title.lower().replace(' ', '_')}_system_overview.html"
    )
    logger.info(f"System Overview HTML Report generated at: {html_file_path}")
    logger.info("--- Stage 1 Pipeline Finished ---")


def run_stage2_detailed_report_pipeline(output_dir: Path, aggregated_db_name: str, report_title: str):
    """
    Orchestrates Stage 2: Detailed Comparative Report.
    This is similar to the previous full pipeline but assumes aggregated DB exists.
    """
    logger = logging.getLogger(__name__)
    logger.info("--- Starting Stage 2: Detailed Comparative Report ---")

    output_dir.mkdir(parents=True, exist_ok=True)
    aggregated_db_path = output_dir / aggregated_db_name
    glossary_path = DEFAULT_GLOSSARY_PATH
    plots_output_dir = output_dir / "plots"
    plots_output_dir.mkdir(parents=True, exist_ok=True)

    if not aggregated_db_path.exists():
        logger.error(f"Aggregated database {aggregated_db_path} not found. "
                       "Run Stage 1 or ensure DB is present before running Stage 2.")
        return

    # Ensure dummy files for a smooth MVP run
    ensure_dummy_glossary(glossary_path)
    # For Stage 2, we assume a more comprehensive report_template.html will be developed/used.
    # For now, let's make sure a default one (potentially the system_overview one if no other) exists.
    ensure_system_overview_template(DEFAULT_TEMPLATES_DIR, "report_template.html") # Or ensure a proper detailed template
    ensure_assets(DEFAULT_ASSETS_DIR)

    logger.info("Initializing components...")
    data_repo = DataRepository(db_path=aggregated_db_path)
    priority_metrics_list = ["DW_PROA_Open_Fraction", "Ion_HMM_ConductionEvents_Total"] # Example
    analysis_filter = AnalysisFilter(priority_metrics=priority_metrics_list)
    section_builder = SectionBuilder(glossary_file_path=glossary_path)
    ai_generator = MockAIInsightGenerator(glossary_csv_path=glossary_path)
    viz_engine = VisualizationEngine(output_plots_dir=plots_output_dir)
    report_renderer = ReportLayoutRenderer(
        templates_dir=DEFAULT_TEMPLATES_DIR,
        output_dir=output_dir,
        assets_dir=DEFAULT_ASSETS_DIR
    )

    logger.info("Loading and comparing metrics for detailed report...")
    run_meta = data_repo.load_run_metadata()
    all_compared_metrics = data_repo.load_and_compare_metrics() # This is the key call for Stage 2
    if not all_compared_metrics:
        logger.error("No compared metrics loaded for detailed report. Aborting.")
        return

    logger.info(f"Filtering {len(all_compared_metrics)} compared metrics...")
    significant_metrics = analysis_filter.filter_metrics(all_compared_metrics)

    logger.info("Grouping metrics into sections...")
    report_sections_no_plots_ai = section_builder.group_metrics_into_sections(significant_metrics)

    final_report_sections = []
    for section_data in report_sections_no_plots_ai:
        logger.info(f"Processing section: {section_data.title}")
        plot_paths = viz_engine.generate_plots_for_section(section_data)
        section_data.plots = [p.relative_to(output_dir) for p in plot_paths]
        section_data.ai_interpretation = ai_generator.generate_section_insight(section_data, run_meta)
        final_report_sections.append(section_data)

    global_summary_text = None
    if final_report_sections:
        logger.info("Generating global AI summary...")
        global_summary_text = ai_generator.generate_global_summary(final_report_sections, run_meta)

    logger.info("Rendering detailed HTML report...")
    html_file_path = report_renderer.render_html_report(
        report_title=report_title,
        sections=final_report_sections,
        run_metadata=run_meta,
        global_ai_summary=global_summary_text,
        template_name="report_template.html", # Assuming this will be developed for Stage 2
        output_filename=f"{report_title.lower().replace(' ', '_')}_detailed_report.html"
    )
    logger.info(f"Detailed HTML Report generated at: {html_file_path}")

    pdf_file_path = report_renderer.render_pdf_report(html_report_path=html_file_path)
    if pdf_file_path:
        logger.info(f"PDF Report generated at: {pdf_file_path}")

    logger.info("--- Detailed Report Pipeline Finished ---")


if __name__ == "__main__":
    setup_logging()
    parser = argparse.ArgumentParser(description="Enhanced Report Generation System.")
    parser.add_argument(
        "--output_dir", type=Path, default=DEFAULT_OUTPUT_DIR,
        help="Directory to save the generated report and plots."
    )
    parser.add_argument(
        "--aggregated_db_name", type=str, default=DEFAULT_AGGREGATED_DB_NAME,
        help="Name of the SQLite database file (e.g., enhanced_cross_analysis.db) within the output directory."
    )
    parser.add_argument(
        "--title", type=str, default="Cross-Simulation Analysis",
        help="Base title for the generated report."
    )
    parser.add_argument(
        "--stage", type=str, choices=['aggregate', 'overview', 'detailed'], default='overview',
        help=("Specify pipeline stage: "
              "'aggregate' to only run data aggregation, "
              "'overview' to run aggregation (if --sim_roots provided) and generate system overview, "
              "'detailed' to generate the full detailed report (assumes aggregated DB exists or is created by 'overview').")
    )
    parser.add_argument(
        "--sim_roots", type=Path, nargs='*',
        help=("Root directories containing individual simulation run folders (each with an analysis_registry.db). "
              "Required if running 'aggregate' or 'overview' for the first time.")
    )
    parser.add_argument(
        "--force_aggregate", action='store_true',
        help="Force re-aggregation of data even if --stage is 'overview' and DB exists."
    )

    args = parser.parse_args()

    try:
        if args.stage == 'aggregate' or args.stage == 'overview' or args.force_aggregate:
            if not args.sim_roots and not (args.output_dir / args.aggregated_db_name).exists():
                parser.error("--sim_roots are required for initial aggregation or if the aggregated DB doesn't exist.")
            if args.sim_roots: # Only pass sim_roots if provided, to avoid error in run_stage1_pipeline
                 run_stage1_pipeline(args.output_dir, args.aggregated_db_name, args.title, args.sim_roots)
            elif (args.output_dir / args.aggregated_db_name).exists() and args.stage == 'overview':
                 # If DB exists and we just want overview without re-aggregating
                 run_stage1_pipeline(args.output_dir, args.aggregated_db_name, args.title, [])
            elif not args.sim_roots and args.stage == 'aggregate':
                 logger.warning("Stage 'aggregate' chosen but no --sim_roots provided. Assuming DB already aggregated.")


        if args.stage == 'detailed':
            run_stage2_detailed_report_pipeline(args.output_dir, args.aggregated_db_name, args.title)
        elif args.stage == 'aggregate':
            logger.info("Data aggregation stage finished (or skipped if no --sim_roots).")

    except Exception as e:
        logging.exception("Pipeline failed with an error:")

5. Update other components (briefly):
enhanced_report_generator/core/data_models.py: No major changes needed for Stage 1 MVP, the existing MetricResult, ReportSection, RunMetadata should suffice.
enhanced_report_generator/components/analysis_selection.py:
AnalysisFilter: Will not be used heavily in Stage 1. Its filter_metrics might be called with an empty list or its logic simplified for the overview.
SectionBuilder: group_metrics_into_sections will not be called in Stage 1; the "System Overview" section is manually created in main.py. The glossary might define a "SystemOverview" prefix but it's not strictly necessary for the MVP's Stage 1.
enhanced_report_generator/components/visualization.py: VisualizationEngine will likely not generate any plots for the minimal "System Overview" in Stage 1.
enhanced_report_generator/components/ai_insights.py: MockAIInsightGenerator will provide a placeholder text for the "System Overview". Its generate_section_insight method needs to be robust enough to handle a section with potentially different kinds of metrics (e.g., counts, averages not necessarily toxin vs control yet).
enhanced_report_generator/components/rendering.py: ReportLayoutRenderer will use the new system_overview_template.html for Stage 1.
How to run the MVP (Stage 1):
Set up directory structure:
your_project_root/
├── enhanced_report_generator/
│   ├── __init__.py
│   ├── main.py
│   ├── core/
│   │   ├── __init__.py
│   │   └── data_models.py
│   ├── components/
│   │   ├── __init__.py
│   │   ├── data_aggregator.py
│   │   ├── data_access.py
│   │   ├── analysis_selection.py
│   │   ├── visualization.py
│   │   ├── ai_insights.py
│   │   └── rendering.py
│   ├── templates/
│   │   ├── __init__.py
│   │   └── system_overview_template.html
│   │   └── report_template.html  (can be a copy of system_overview for now or the previous detailed one)
│   ├── config/
│   │   ├── __init__.py
│   │   └── glossary_mapping.csv
│   └── assets/
│       ├── __init__.py
│       └── style.css
└── example_sim_runs/  # Create this directory for testing
    ├── toxin_systems/
    │   └── dummy_run_toxin_systems_1/
    │       └── analysis_registry.db # (will be created by data_aggregator example)
    │   └── ...
    └── control_systems/
        └── dummy_run_control_systems_1/
        │   └── analysis_registry.db # (will be created by data_aggregator example)
        └── ...
Populate __init__.py files in each subdirectory to make them packages (can be empty).
Run the data_aggregator.py script directly first (optional, for testing aggregation):
Bash

python -m enhanced_report_generator.components.data_aggregator
This will create dummy analysis_registry.db files in example_sim_runs (if they don't exist) and then create enhanced_cross_analysis.db in the current directory.
Run main.py for Stage 1 ("overview"):
Bash

python -m enhanced_report_generator.main --stage overview --output_dir ./my_stage1_report --title "SystemAnalysis" --sim_roots ./example_sim_runs/toxin_systems ./example_sim_runs/control_systems
This command should:
Run the data aggregation (because --sim_roots are provided).
Generate ./my_stage1_report/SystemAnalysis_system_overview.html.
The report will contain the "System Overview" tab with high-level data.
This setup provides the scaffolding and the initial aggregated database. From here, Stage 2 would involve a human expert providing instructions to populate glossary_mapping.csv with meaningful section prefixes, enhancing AnalysisFilter rules, adding plotting functions to VisualizationEngine, and iteratively building out the report_template.html for a detailed report by running main.py --stage detailed.
